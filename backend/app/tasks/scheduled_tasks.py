"""å®šæ—¶è°ƒåº¦ä»»åŠ¡ â€” Procrastinate periodic

å¯¹ç…§è„‘å›¾ã€Œä»»åŠ¡è°ƒåº¦ã€:
  å®šæ—¶å™¨ â†’ æ•°æ®æŠ“å– (æ™ºèƒ½è°ƒåº¦, é€€é¿ç­–ç•¥, ä¿¡æ¯æµæŠ“å–)
           å‘¨æœŸä»»åŠ¡ (å†…å®¹åˆ†æ, æ‰¹é‡æŠ¥å‘Š)
  æµæ°´çº¿ â†’ æ•°æ®å¤„ç†

æ ¸å¿ƒæµç¨‹:
  å®šæ—¶å™¨ â†’ CollectionService.collect(source)
         â†’ Collector æŠ“å–åŸå§‹æ•°æ® â†’ å»é‡ â†’ åˆ›å»º ContentItem
         â†’ å¯¹æ¯æ¡æ–°å†…å®¹, æŒ‰ç»‘å®šçš„æµæ°´çº¿æ¨¡æ¿åˆ›å»º PipelineExecution

æ‰€æœ‰å®šæ—¶ä»»åŠ¡ç”± Procrastinate worker çš„ periodic åŠŸèƒ½é©±åŠ¨ã€‚
"""

import logging

from app.tasks.procrastinate_app import proc_app

logger = logging.getLogger(__name__)


@proc_app.periodic(cron="*/1 * * * *")  # æ¯1åˆ†é’Ÿè§¦å‘ï¼Œæ”¯æŒ60ç§’æœ€å°é—´éš”
@proc_app.task(queue="scheduled", queueing_lock="collection_loop")
async def check_and_collect_sources(timestamp):
    """ä¸»é‡‡é›†å¾ªç¯ â€” æ™ºèƒ½è°ƒåº¦ + é€€é¿ç­–ç•¥"""
    import time
    start_time = time.time()  # æ€§èƒ½ç›‘æ§ï¼šè®°å½•å¼€å§‹æ—¶é—´

    from datetime import timedelta
    from app.core.database import SessionLocal
    from app.core.time import utcnow
    from app.models.content import SourceConfig, ContentItem, ContentStatus
    from app.models.pipeline import TriggerSource
    from app.services.pipeline.orchestrator import PipelineOrchestrator
    from app.services.scheduling_service import SchedulingService

    with SessionLocal() as db:
        now = utcnow()
        query_start = time.time()  # æ€§èƒ½ç›‘æ§ï¼šè®°å½•æŸ¥è¯¢å¼€å§‹æ—¶é—´

        # ä½¿ç”¨æ™ºèƒ½è°ƒåº¦ï¼šæŸ¥è¯¢ next_collection_at <= now çš„æº
        # ç´¢å¼•ä¼˜åŒ–ï¼šix_source_next_collection (is_active, schedule_enabled, next_collection_at)
        sources = db.query(SourceConfig).filter(
            SourceConfig.is_active == True,
            SourceConfig.schedule_enabled == True,
            (SourceConfig.next_collection_at.is_(None)) |
            (SourceConfig.next_collection_at <= now)
        ).all()

        query_elapsed = time.time() - query_start
        logger.info(
            f"Collection check: {len(sources)} sources due for collection "
            f"(query took {query_elapsed*1000:.1f}ms)"
        )

        # ---- defer é‡‡é›†ä»»åŠ¡åˆ° worker å¹¶å‘æ‰§è¡Œ ----
        from app.tasks.collection_tasks import collect_single_source
        from app.tasks.procrastinate_app import async_defer

        sources_deferred = 0
        sources_skipped = 0

        for source in sources:
            # äºŒæ¬¡ç¡®è®¤ï¼ˆé˜²æ­¢å¹¶å‘æˆ–æ—¶é—´åç§»ï¼‰
            if not SchedulingService.should_collect_now(source, now):
                sources_skipped += 1
                continue

            try:
                await async_defer(
                    collect_single_source,
                    queueing_lock=f"collect_{source.id}",
                    source_id=source.id,
                    trigger="scheduled",
                    use_retry=True,
                )
                sources_deferred += 1
            except Exception as e:
                logger.debug(f"Collection defer skipped for {source.name} (already queued): {e}")
                sources_skipped += 1

        logger.info(
            f"Collection scheduler: {sources_deferred} deferred, "
            f"{sources_skipped} skipped"
        )

        # ---- è¡¥å¿: å¯¹ pending ä¸”æ—  pipeline çš„å†…å®¹è¡¥è§¦å‘ ----
        from app.models.pipeline import PipelineExecution
        from sqlalchemy import not_, exists

        # é™åˆ¶æ¯æ¬¡è¡¥å¿100æ¡ï¼Œé¿å…1åˆ†é’Ÿå‘¨æœŸä¸‹å•æ¬¡æŸ¥è¯¢è¿‡å¤šï¼ˆé¢‘ç‡å¢åŠ 5å€ï¼‰
        orphaned = db.query(ContentItem).filter(
            ContentItem.status == ContentStatus.PENDING.value,
            ~exists().where(PipelineExecution.content_id == ContentItem.id),
        ).limit(100).all()

        if orphaned:
            logger.info(f"Compensation: {len(orphaned)} pending items without pipeline")
            orchestrator = PipelineOrchestrator(db)
            for item in orphaned:
                try:
                    execution = orchestrator.trigger_for_content(
                        content=item,
                        trigger=TriggerSource.SCHEDULED,
                    )
                    if execution:
                        await orchestrator.async_start_execution(execution.id)
                        logger.info(f"Compensation pipeline: {item.title[:50]} â†’ {execution.template_name}")
                except Exception as e:
                    db.rollback()
                    logger.error(f"Compensation trigger failed for {item.title[:50]}: {e}")
            db.commit()

        # ---- æ¢å¤: å¡åœ¨ running è¶…æ—¶çš„æ­¥éª¤é‡æ–°å…¥é˜Ÿ ----
        from app.models.pipeline import PipelineStep, StepStatus, PipelineStatus

        stale_timeout = timedelta(minutes=30)
        stale_steps = db.query(PipelineStep).filter(
            PipelineStep.status == StepStatus.RUNNING.value,
            PipelineStep.started_at < now - stale_timeout,
        ).all()

        steps_to_requeue = []
        for step in stale_steps:
            step.status = StepStatus.PENDING.value
            step.started_at = None
            step.error_message = None
            execution = db.get(PipelineExecution, step.pipeline_id)
            if execution and execution.status == PipelineStatus.RUNNING.value:
                logger.info(f"Recovering stale step: {step.step_type} (pipeline={step.pipeline_id[:12]})")
                steps_to_requeue.append((step.pipeline_id, step.step_index))

        if stale_steps:
            db.commit()
            # é‡æ–°å…¥é˜Ÿï¼ˆcommit åå†å…¥é˜Ÿï¼Œç¡®ä¿çŠ¶æ€å·²æŒä¹…åŒ–ï¼‰
            from app.tasks.pipeline_tasks import execute_pipeline_step
            from app.tasks.procrastinate_app import async_defer
            for pipeline_id, step_index in steps_to_requeue:
                await async_defer(execute_pipeline_step, execution_id=pipeline_id, step_index=step_index)

        # ---- æ¢å¤: execution å¡ä½ (defer å¤±è´¥æˆ– worker å´©æºƒ) ----
        # æƒ…å†µ1: PENDING è¶…è¿‡ 10 åˆ†é’Ÿ â€” sync_defer å¯èƒ½å¤±è´¥, é‡æ–°å…¥é˜Ÿ
        # æƒ…å†µ2: RUNNING ä½†å½“å‰æ­¥éª¤ä» pending â€” worker å´©æºƒåæœªæ¨è¿›
        stuck_execs = db.query(PipelineExecution).filter(
            (
                (PipelineExecution.status == PipelineStatus.PENDING.value)
                & (PipelineExecution.created_at < now - timedelta(minutes=10))
            ) | (
                (PipelineExecution.status == PipelineStatus.RUNNING.value)
                & (PipelineExecution.started_at < now - timedelta(minutes=2))
            )
        ).all()

        stuck_to_requeue = []
        for ex in stuck_execs:
            current_step = db.query(PipelineStep).filter(
                PipelineStep.pipeline_id == ex.id,
                PipelineStep.step_index == ex.current_step,
                PipelineStep.status == StepStatus.PENDING.value,
            ).first()
            if current_step:
                stuck_to_requeue.append((ex.id, ex.current_step))

        if stuck_to_requeue:
            logger.info(f"Recovering {len(stuck_to_requeue)} stuck executions (pending step, defer likely failed)")
            from app.tasks.pipeline_tasks import execute_pipeline_step
            from app.tasks.procrastinate_app import async_defer
            for pipeline_id, step_index in stuck_to_requeue:
                try:
                    await async_defer(execute_pipeline_step, execution_id=pipeline_id, step_index=step_index)
                except Exception as e:
                    logger.error(f"Failed to requeue stuck step: pipeline={pipeline_id[:12]}, error={e}")

        # ---- æ€§èƒ½ç›‘æ§: è®°å½•ä¸»å¾ªç¯æ€»è€—æ—¶ ----
        total_elapsed = time.time() - start_time
        logger.info(f"Collection loop completed in {total_elapsed:.2f}s")
        if total_elapsed > 55:  # æ¥è¿‘1åˆ†é’Ÿè¶…æ—¶ï¼ˆç•™5ç§’ä½™é‡ï¼‰
            logger.warning(
                f"Collection loop near timeout: {total_elapsed:.2f}s "
                f"(processed {len(sources)} sources)"
            )


@proc_app.periodic(cron="0 22 * * *")
@proc_app.task(queue="scheduled", queueing_lock="daily_report")
async def trigger_daily_report(timestamp):
    """æ—¥æŠ¥ â€” æ¯å¤© 22:00"""
    from app.tasks.report_tasks import generate_daily_report
    await generate_daily_report()


@proc_app.periodic(cron="0 9 * * 1")
@proc_app.task(queue="scheduled", queueing_lock="weekly_report")
async def trigger_weekly_report(timestamp):
    """å‘¨æŠ¥ â€” æ¯å‘¨ä¸€ 09:00"""
    from app.tasks.report_tasks import generate_weekly_report
    await generate_weekly_report()


@proc_app.periodic(cron="0 4 * * *")
@proc_app.task(queue="scheduled", queueing_lock="analyze_periodicity")
async def analyze_source_periodicity(timestamp):
    """å‘¨æœŸæ€§åˆ†æä»»åŠ¡ â€” æ¯å¤©å‡Œæ™¨ 4 ç‚¹åˆ†ææ‰€æœ‰æ´»è·ƒæºçš„æ›´æ–°æ¨¡å¼"""
    import json
    from app.core.database import SessionLocal
    from app.core.time import utcnow
    from app.models.content import SourceConfig
    from app.services.scheduling_service import SchedulingService

    with SessionLocal() as db:
        sources = db.query(SourceConfig).filter(
            SourceConfig.is_active == True,
            SourceConfig.schedule_mode == "auto",
        ).all()

        analyzed_count = 0
        for source in sources:
            try:
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°ï¼ˆè‡³å°‘24å°æ—¶ä¸€æ¬¡ï¼‰
                if source.periodicity_updated_at:
                    hours_since = (utcnow() - source.periodicity_updated_at).total_seconds() / 3600
                    if hours_since < 24:
                        continue

                # æ‰§è¡Œå‘¨æœŸæ€§åˆ†æ
                periodicity = SchedulingService.analyze_periodicity(source, db)

                # ä¿å­˜ç»“æœ
                if periodicity["pattern_type"] != "none":
                    source.periodicity_data = json.dumps(periodicity)
                    logger.info(
                        f"Periodicity detected: {source.name} - {periodicity['pattern_type']} "
                        f"(confidence={periodicity['confidence']:.2f})"
                    )
                else:
                    source.periodicity_data = None

                source.periodicity_updated_at = utcnow()
                analyzed_count += 1

            except Exception as e:
                logger.error(f"Periodicity analysis failed for {source.name}: {e}")

        db.commit()
        logger.info(f"Periodicity analysis complete: {analyzed_count}/{len(sources)} sources analyzed")


@proc_app.periodic(cron="0 * * * *")
@proc_app.task(queue="scheduled", queueing_lock="cleanup_scheduler")
async def cleanup_scheduler(timestamp):
    """æ¸…ç†ä»»åŠ¡è°ƒåº¦å™¨ â€” æ¯å°æ—¶æ£€æŸ¥,æ ¹æ®é…ç½®åŠ¨æ€æ‰§è¡Œæ¸…ç†"""
    from app.core.time import utcnow
    from app.core.database import SessionLocal
    from app.models.system_setting import SystemSetting

    with SessionLocal() as db:
        # è¯»å–é…ç½®çš„æ¸…ç†æ—¶é—´ï¼ˆæ ¼å¼ï¼šHH:MMï¼‰
        content_time_setting = db.get(SystemSetting, "cleanup_content_time")
        records_time_setting = db.get(SystemSetting, "cleanup_records_time")

        content_cleanup_time = content_time_setting.value if content_time_setting else "03:00"
        records_cleanup_time = records_time_setting.value if records_time_setting else "03:30"

        # è·å–å½“å‰ UTC æ—¶é—´
        now = utcnow()
        current_time = f"{now.hour:02d}:{now.minute:02d}"

        # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾å†…å®¹æ¸…ç†æ—¶é—´
        if should_run_cleanup(current_time, content_cleanup_time):
            last_run_key = "cleanup_content_last_run"
            last_run = db.get(SystemSetting, last_run_key)
            today = now.strftime("%Y-%m-%d")

            if not last_run or last_run.value != today:
                logger.info(f"Running content cleanup at configured time: {content_cleanup_time} UTC")
                await cleanup_expired_content()

                # æ›´æ–°æœ€åæ‰§è¡Œæ—¥æœŸ
                if last_run:
                    last_run.value = today
                else:
                    db.add(SystemSetting(key=last_run_key, value=today))
                db.commit()

        # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾è®°å½•æ¸…ç†æ—¶é—´
        if should_run_cleanup(current_time, records_cleanup_time):
            last_run_key = "cleanup_records_last_run"
            last_run = db.get(SystemSetting, last_run_key)
            today = now.strftime("%Y-%m-%d")

            if not last_run or last_run.value != today:
                logger.info(f"Running records cleanup at configured time: {records_cleanup_time} UTC")
                await cleanup_records()

                if last_run:
                    last_run.value = today
                else:
                    db.add(SystemSetting(key=last_run_key, value=today))
                db.commit()


def should_run_cleanup(current_time: str, target_time: str) -> bool:
    """åˆ¤æ–­å½“å‰æ—¶é—´æ˜¯å¦åŒ¹é…ç›®æ ‡æ¸…ç†æ—¶é—´ï¼ˆå…è®¸Â±30åˆ†é’Ÿè¯¯å·®ï¼‰"""
    try:
        current_h, current_m = map(int, current_time.split(':'))
        target_h, target_m = map(int, target_time.split(':'))

        current_minutes = current_h * 60 + current_m
        target_minutes = target_h * 60 + target_m

        # å…è®¸Â±30åˆ†é’Ÿè¯¯å·®ï¼ˆå› ä¸ºè°ƒåº¦å™¨æ¯å°æ—¶è¿è¡Œä¸€æ¬¡ï¼‰
        return abs(current_minutes - target_minutes) <= 30
    except Exception:
        logger.warning(f"Failed to parse cleanup time: current='{current_time}', target='{target_time}'")
        return False


async def cleanup_expired_content():
    """å®šæ—¶æ¸…ç†è¿‡æœŸå†…å®¹

    é€»è¾‘:
    1. è¯»å–å…¨å±€é»˜è®¤ default_retention_days
    2. éå†æ‰€æœ‰ auto_cleanup_enabled=True çš„æ•°æ®æº
    3. åˆ é™¤è¶…è¿‡ä¿ç•™æœŸä¸”æœªæ”¶è—ã€æ— ç¬”è®°çš„å†…å®¹ï¼ˆçº§è”åˆ é™¤å…³è”æ•°æ®+åª’ä½“æ–‡ä»¶ï¼‰
    """
    import shutil
    from pathlib import Path
    from datetime import timedelta
    from app.core.database import SessionLocal
    from app.core.config import settings
    from app.core.time import utcnow
    from app.models.content import SourceConfig, ContentItem, MediaItem
    from app.models.pipeline import PipelineExecution, PipelineStep
    from app.models.system_setting import SystemSetting

    with SessionLocal() as db:
        # è¯»å–å…¨å±€é»˜è®¤ä¿ç•™å¤©æ•°
        setting = db.get(SystemSetting, "default_retention_days")
        global_retention = int(setting.value) if setting and setting.value else 30
        if global_retention <= 0:
            logger.info("Cleanup skipped: global retention is 0 (keep forever)")
            return

        sources = db.query(SourceConfig).all()

        if not sources:
            logger.info("Cleanup: no sources found")
            return

        now = utcnow()
        total_deleted = 0

        for source in sources:
            retention = source.retention_days if source.retention_days and source.retention_days > 0 else global_retention
            cutoff = now - timedelta(days=retention)

            # æŸ¥æ‰¾è¿‡æœŸä¸”æœªä¿æŠ¤çš„å†…å®¹
            expired_items = db.query(ContentItem).filter(
                ContentItem.source_id == source.id,
                ContentItem.collected_at < cutoff,
                ContentItem.is_favorited == False,
                ContentItem.user_note.is_(None),
            ).all()

            if not expired_items:
                continue

            item_ids = [item.id for item in expired_items]

            # çº§è”åˆ é™¤: steps â†’ executions â†’ media_items â†’ items
            execution_ids = [
                eid for (eid,) in db.query(PipelineExecution.id)
                .filter(PipelineExecution.content_id.in_(item_ids))
                .all()
            ]
            if execution_ids:
                db.query(PipelineStep).filter(
                    PipelineStep.pipeline_id.in_(execution_ids)
                ).delete(synchronize_session=False)
                db.query(PipelineExecution).filter(
                    PipelineExecution.id.in_(execution_ids)
                ).delete(synchronize_session=False)

            db.query(MediaItem).filter(
                MediaItem.content_id.in_(item_ids)
            ).delete(synchronize_session=False)

            deleted = db.query(ContentItem).filter(
                ContentItem.id.in_(item_ids)
            ).delete(synchronize_session=False)

            db.commit()

            # æ¸…ç†åª’ä½“æ–‡ä»¶
            media_base = Path(settings.MEDIA_DIR)
            audio_base = media_base / "audio"
            for item_id in item_ids:
                for d in [media_base / item_id, audio_base / item_id]:
                    if d.is_dir():
                        shutil.rmtree(d, ignore_errors=True)

            total_deleted += deleted
            logger.info(f"Cleanup [{source.name}]: deleted {deleted} expired items (retention={retention}d)")

        logger.info(f"Cleanup complete: {total_deleted} items deleted from {len(sources)} sources")


async def cleanup_records():
    """å®šæ—¶æ¸…ç†æ‰§è¡Œè®°å½•å’Œé‡‡é›†è®°å½•

    é€»è¾‘:
    1. è¯»å– 4 ä¸ª setting key (retention_days + max_count)
    2. æŒ‰å¤©æ•°æ¸…ç†: åˆ é™¤ created_at/started_at æ—©äº cutoff çš„å·²ç»ˆæ€è®°å½•
    3. æŒ‰æ•°é‡æ¸…ç†: æŒ‰æ—¶é—´å€’åºä¿ç•™å‰ N æ¡ï¼Œåˆ é™¤å¤šä½™çš„å·²ç»ˆæ€è®°å½•
    """
    from datetime import timedelta
    from app.core.database import SessionLocal
    from app.core.time import utcnow
    from app.models.system_setting import SystemSetting
    from app.models.pipeline import PipelineExecution, PipelineStep, PipelineStatus
    from app.models.content import CollectionRecord

    with SessionLocal() as db:
        # è¯»å–è®¾ç½®
        def get_setting(key: str, default: int) -> int:
            row = db.get(SystemSetting, key)
            if row and row.value:
                try:
                    return int(row.value)
                except ValueError:
                    pass
            return default

        exec_retention_days = get_setting("execution_retention_days", 30)
        exec_max_count = get_setting("execution_max_count", 0)
        coll_retention_days = get_setting("collection_retention_days", 30)
        coll_max_count = get_setting("collection_max_count", 0)

        now = utcnow()
        total_exec_deleted = 0
        total_coll_deleted = 0

        # ---- æ‰§è¡Œè®°å½•æ¸…ç† ----
        exec_terminal = [
            PipelineStatus.COMPLETED.value,
            PipelineStatus.FAILED.value,
            PipelineStatus.CANCELLED.value,
        ]

        # æŒ‰å¤©æ•°æ¸…ç†
        if exec_retention_days > 0:
            cutoff = now - timedelta(days=exec_retention_days)
            exec_ids = [
                eid for (eid,) in db.query(PipelineExecution.id).filter(
                    PipelineExecution.status.in_(exec_terminal),
                    PipelineExecution.created_at < cutoff,
                ).all()
            ]
            if exec_ids:
                db.query(PipelineStep).filter(
                    PipelineStep.pipeline_id.in_(exec_ids)
                ).delete(synchronize_session=False)
                deleted = db.query(PipelineExecution).filter(
                    PipelineExecution.id.in_(exec_ids)
                ).delete(synchronize_session=False)
                total_exec_deleted += deleted

        # æŒ‰æ•°é‡æ¸…ç†
        if exec_max_count > 0:
            # æŒ‰åˆ›å»ºæ—¶é—´å€’åº, è·³è¿‡å‰ N æ¡, åˆ é™¤å‰©ä½™çš„å·²ç»ˆæ€è®°å½•
            overflow_ids = [
                eid for (eid,) in db.query(PipelineExecution.id).filter(
                    PipelineExecution.status.in_(exec_terminal),
                ).order_by(PipelineExecution.created_at.desc()).offset(exec_max_count).all()
            ]
            if overflow_ids:
                db.query(PipelineStep).filter(
                    PipelineStep.pipeline_id.in_(overflow_ids)
                ).delete(synchronize_session=False)
                deleted = db.query(PipelineExecution).filter(
                    PipelineExecution.id.in_(overflow_ids)
                ).delete(synchronize_session=False)
                total_exec_deleted += deleted

        # ---- é‡‡é›†è®°å½•æ¸…ç†ï¼ˆå¸¦ä¿æŠ¤æœºåˆ¶ï¼‰----
        coll_terminal = ["completed", "failed"]

        # ğŸ†• æ¯ä¸ªæ•°æ®æºè‡³å°‘ä¿ç•™çš„è®°å½•æ•°
        MIN_KEEP_PER_SOURCE = get_setting("collection_min_keep", 10)

        # æŒ‰å¤©æ•°æ¸…ç†ï¼ˆä¿æŠ¤æœ€æ–°è®°å½•ï¼‰
        if coll_retention_days > 0:
            cutoff = now - timedelta(days=coll_retention_days)

            # ğŸ†• æŒ‰æ•°æ®æºåˆ†åˆ«å¤„ç†ï¼Œç¡®ä¿æ¯ä¸ªæºè‡³å°‘ä¿ç•™ MIN_KEEP_PER_SOURCE æ¡
            from app.models.content import SourceConfig
            sources = db.query(SourceConfig).all()

            for source in sources:
                # æŸ¥è¯¢è¯¥æºçš„æ‰€æœ‰ç»ˆæ€è®°å½•ï¼ŒæŒ‰æ—¶é—´å€’åº
                all_records = db.query(CollectionRecord).filter(
                    CollectionRecord.source_id == source.id,
                    CollectionRecord.status.in_(coll_terminal),
                ).order_by(CollectionRecord.started_at.desc()).all()

                # ä¿æŠ¤æœ€æ–°çš„ MIN_KEEP_PER_SOURCE æ¡è®°å½•
                if len(all_records) <= MIN_KEEP_PER_SOURCE:
                    continue  # æ€»æ•°ä¸è¶…è¿‡æœ€å°ä¿ç•™æ•°ï¼Œè·³è¿‡æ¸…ç†

                # åªåˆ é™¤è¿‡æœŸä¸”ä¸åœ¨ä¿æŠ¤èŒƒå›´å†…çš„è®°å½•
                deletable_ids = [
                    r.id for r in all_records[MIN_KEEP_PER_SOURCE:]
                    if r.started_at < cutoff
                ]

                if deletable_ids:
                    deleted = db.query(CollectionRecord).filter(
                        CollectionRecord.id.in_(deletable_ids)
                    ).delete(synchronize_session=False)
                    total_coll_deleted += deleted
                    logger.info(
                        f"Cleanup [{source.name}]: deleted {deleted} collection records "
                        f"(kept {MIN_KEEP_PER_SOURCE} most recent)"
                    )

        db.commit()
        logger.info(
            f"Records cleanup: {total_exec_deleted} executions, "
            f"{total_coll_deleted} collection records deleted. "
            f"Protected records per source: {MIN_KEEP_PER_SOURCE}"
        )

        # ğŸ†• å¤§é‡åˆ é™¤è­¦å‘Š
        if total_coll_deleted > 100:
            logger.warning(
                f"Large cleanup detected: {total_coll_deleted} collection records deleted. "
                f"Please check retention settings."
            )
